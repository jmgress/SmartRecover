# LLM Configuration for SmartRecover
# Specify which LLM provider to use: "openai", "gemini", or "ollama"

llm:
  provider: "ollama"  # Options: "openai", "gemini", "ollama"
  
  # OpenAI Configuration
  openai:
    model: "gpt-3.5-turbo"  # Options: gpt-3.5-turbo, gpt-4, gpt-4-turbo-preview, etc.
    temperature: 0.7
    # API key can be set here or via OPENAI_API_KEY environment variable
    # api_key: "your-api-key-here"
  
  # Google Gemini Configuration
  gemini:
    model: "gemini-pro"  # Options: gemini-pro, gemini-pro-vision
    temperature: 0.7
    # API key can be set here or via GOOGLE_API_KEY environment variable
    # api_key: "your-api-key-here"
  
  # Ollama Configuration (for local LLMs)
  ollama:
    model: "llama3.2"  # Options: llama2, mistral, codellama, etc.
    base_url: "http://localhost:11434"  # Default Ollama endpoint
    temperature: 0.7

# Logging Configuration
logging:
  level: "INFO"  # Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  enable_tracing: false  # Enable detailed function tracing (use DEBUG level for trace output)
  # log_file: "logs/smartrecover.log"  # Optional: log to file (uncomment to enable)
