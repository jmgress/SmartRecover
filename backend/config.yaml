# LLM Configuration for SmartRecover
# Specify which LLM provider to use: "openai", "gemini", or "ollama"

llm:
  provider: "openai"  # Options: "openai", "gemini", "ollama"
  
  # OpenAI Configuration
  openai:
    model: "gpt-3.5-turbo"  # Options: gpt-3.5-turbo, gpt-4, gpt-4-turbo-preview, etc.
    temperature: 0.7
    # API key can be set here or via OPENAI_API_KEY environment variable
    # api_key: "your-api-key-here"
  
  # Google Gemini Configuration
  gemini:
    model: "gemini-pro"  # Options: gemini-pro, gemini-pro-vision
    temperature: 0.7
    # API key can be set here or via GOOGLE_API_KEY environment variable
    # api_key: "your-api-key-here"
  
  # Ollama Configuration (for local LLMs)
  ollama:
    model: "llama2"  # Options: llama2, mistral, codellama, etc.
    base_url: "http://localhost:11434"  # Default Ollama endpoint
    temperature: 0.7
