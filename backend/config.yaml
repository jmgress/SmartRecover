# LLM Configuration for SmartRecover
# Specify which LLM provider to use: "openai", "gemini", or "ollama"

llm:
  provider: "ollama"  # Options: "openai", "gemini", "ollama"
  
  # OpenAI Configuration
  openai:
    model: "gpt-3.5-turbo"  # Options: gpt-3.5-turbo, gpt-4, gpt-4-turbo-preview, etc.
    temperature: 0.7
    # API key can be set here or via OPENAI_API_KEY environment variable
    # api_key: "your-api-key-here"
  
  # Google Gemini Configuration
  gemini:
    model: "gemini-pro"  # Options: gemini-pro, gemini-pro-vision
    temperature: 0.7
    # API key can be set here or via GOOGLE_API_KEY environment variable
    # api_key: "your-api-key-here"
  
  # Ollama Configuration (for local LLMs)
  ollama:
    model: "llama3.2"  # Options: llama2, mistral, codellama, etc.
    base_url: "http://localhost:11434"  # Default Ollama endpoint
    temperature: 0.7

# Logging Configuration
logging:
  level: "INFO"  # Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  enable_tracing: false  # Enable detailed function tracing (use DEBUG level for trace output)
  # log_file: "logs/smartrecover.log"  # Optional: log to file (uncomment to enable)

# Knowledge Base Configuration
knowledge_base:
  source: "mock"  # Options: "mock", "confluence"
  
  # Confluence Configuration (for production)
  confluence:
    base_url: ""  # e.g., "https://your-domain.atlassian.net/wiki"
    username: ""  # Confluence username/email
    api_token: ""  # Confluence API token
    space_keys: []  # Optional: list of space keys to search, e.g., ["DOCS", "KB"]
  
  # Mock Configuration (for development/testing)
  mock:
    csv_path: "backend/data/csv/confluence_docs.csv"
    docs_folder: null  # Optional: path to folder with .md/.txt files, e.g., "backend/data/runbooks/"
