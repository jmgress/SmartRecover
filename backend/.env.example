# LLM Provider Configuration
# Specify which provider to use: "openai", "gemini", or "ollama"
LLM_PROVIDER=openai

# OpenAI Configuration
OPENAI_API_KEY=your-openai-api-key-here
OPENAI_MODEL=gpt-3.5-turbo

# Google Gemini Configuration
GOOGLE_API_KEY=your-google-api-key-here
GEMINI_MODEL=gemini-pro

# Ollama Configuration (for local LLMs)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama2

# Logging Configuration
LOG_LEVEL=INFO  # Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
# LOG_FORMAT=%(asctime)s - %(name)s - %(levelname)s - %(message)s
ENABLE_TRACING=false  # Set to true to enable detailed function tracing (use DEBUG level for trace output)
# LOG_FILE=logs/smartrecover.log  # Uncomment to enable logging to file
